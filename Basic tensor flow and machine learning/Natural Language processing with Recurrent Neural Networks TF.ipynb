{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPisH/GONLWo2TmdrBTSnut"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"uwo_Il8sUwMG"},"outputs":[],"source":["#Natural Language processing (NLP) is the field or discipline in computing, or machine learning\n","#that deals with trying to understand natural human languages, spellcheck, autocomplete, voice assistants, translation\n","#Recurrent Neural Networks(RNN) is much more capable of processing sequential datasuch as text or characters\n","#Sequence Data\n","#Need to convert the text data to numeric"]},{"cell_type":"code","source":["#Bag of words algorithm: every single word in the vocabulary is going to be placed in a dictionary and have some integer that represents it\n","#only going to keep track of the words that are present and the frequency of those words, you ose  the order of the words\n","#this metod lose hte context of the words within the sentence,very different meaning they are encoded exaclty the same way"],"metadata":{"id":"l8dSRTtVZI5t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#This isn't really the way we would do this in practice, it gives an idea of how bag of words works. \n","vocab = {}  # maps word to integer representing it\n","word_encoding = 1\n","def bag_of_words(text):\n","  global word_encoding\n","\n","  words = text.lower().split(\" \")  # create a list of all of the words in the text, well assume there is no grammar in our text for this example\n","  bag = {}  # stores all of the encodings and their frequency\n","\n","  for word in words:\n","    if word in vocab:\n","      encoding = vocab[word]  # get encoding from vocab\n","    else:\n","      vocab[word] = word_encoding\n","      encoding = word_encoding\n","      word_encoding += 1\n","    \n","    if encoding in bag:\n","      bag[encoding] += 1\n","    else:\n","      bag[encoding] = 1\n","  \n","  return bag\n","\n","text = \"this is a test to see if this test will work is is test a a\"\n","bag = bag_of_words(text)\n","print(bag)\n","print(vocab)\n"],"metadata":{"id":"BaoA5-wwZKgi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#words embedding\n","#try to find a wat to represent words that are similar using very similar numbers\n","#it classify or translate every single word into a vector\n","#very similar vectors are similar words, and opposite are far away\n","#the word embeddings are actually trained, and learns these word embeddings as it goes, so we can add it layer to give a vector that implies the context of the word in a sentence\n"],"metadata":{"id":"apvcYcn-Zvfk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Recurren neural network (RNN)\n","#the diference is that it contains an internal loop\n","#doesnt process the entire input data at once, processes it at different time steps\n","#mantains an internal memory of an internal state, the new input will be trated based on the previous input and its context\n","#procces one word at a time, generate output based in it, and save internal memory state that is keeping track to do the calculation\n","#the output from 1 word will be added to the next input (simple recurrent layer)\n","#large sentences implies inscreasingly difficult fot the model to actually build a good understanding\n","#LST (long short term memory, the next layer)\n","#add another component that keep tracking the internal state\n","#add all the outputs of that layer, allows use all these values\n"],"metadata":{"id":"7cqHM8VRBzZP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Sentiment analysis\n","%tensorflow_version 2.x  # this line is not required unless you are in a notebook\n","from keras.datasets import imdb\n","from keras.preprocessing import sequence\n","import keras\n","import tensorflow as tf\n","import os\n","import numpy as np\n","\n","VOCAB_SIZE = 88584\n","#0 more common, 88584 less common\n","MAXLEN = 250\n","BATCH_SIZE = 64\n","\n","(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = VOCAB_SIZE)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O6y4iSLDNvCi","executionInfo":{"status":"ok","timestamp":1676484880188,"user_tz":180,"elapsed":9233,"user":{"displayName":"santiago lucero","userId":"03286679709682630982"}},"outputId":"ed38ba2d-5e5a-4b72-e3b6-ffa0c8c489ea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","17464789/17464789 [==============================] - 2s 0us/step\n"]}]},{"cell_type":"code","source":["# Lets look at one review\n","train_data[1]\n"],"metadata":{"id":"Xzk03T85N7uQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#If we have a look at some of our loaded in reviews, we'll notice that they are different lengths. This is an issue. We cannot pass different length data into our neural network. Therefore, we must make each review the same length. \n","from keras.utils import pad_sequences\n","#trim extra words and add the necessary amount of 0's to make it equal to maxlen\n","train_data = pad_sequences(train_data, MAXLEN)\n","test_data = pad_sequences(test_data, MAXLEN)  "],"metadata":{"id":"O9TvvqtKOgP_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Create the model\n","\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(VOCAB_SIZE, 32), #embedding\n","    tf.keras.layers.LSTM(32), #lstm\n","    tf.keras.layers.Dense(1, activation=\"sigmoid\") #dense with activation\n","]) #output between 0 and 1"],"metadata":{"id":"QzfQvXyJPxy3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"t5sD9dYVRySN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Training\n","\n","model.compile(loss=\"binary_crossentropy\",optimizer=\"rmsprop\",metrics=['acc'])\n","\n","history = model.fit(train_data, train_labels, epochs=7, validation_split=0.2) #use the 0.2 % of the training data to validate the model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yVbu5g62R6Qs","executionInfo":{"status":"ok","timestamp":1676485054418,"user_tz":180,"elapsed":142552,"user":{"displayName":"santiago lucero","userId":"03286679709682630982"}},"outputId":"ee0036e6-1459-455d-f03a-2bf2ebfd6cdc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/7\n","625/625 [==============================] - 50s 77ms/step - loss: 0.4040 - acc: 0.8213 - val_loss: 0.2955 - val_acc: 0.8816\n","Epoch 2/7\n","625/625 [==============================] - 18s 29ms/step - loss: 0.2466 - acc: 0.9064 - val_loss: 0.2996 - val_acc: 0.8880\n","Epoch 3/7\n","625/625 [==============================] - 13s 20ms/step - loss: 0.1944 - acc: 0.9289 - val_loss: 0.3194 - val_acc: 0.8620\n","Epoch 4/7\n","625/625 [==============================] - 11s 17ms/step - loss: 0.1570 - acc: 0.9449 - val_loss: 0.3420 - val_acc: 0.8838\n","Epoch 5/7\n","625/625 [==============================] - 13s 21ms/step - loss: 0.1321 - acc: 0.9547 - val_loss: 0.3329 - val_acc: 0.8620\n","Epoch 6/7\n","625/625 [==============================] - 12s 19ms/step - loss: 0.1081 - acc: 0.9639 - val_loss: 0.3420 - val_acc: 0.8610\n","Epoch 7/7\n","625/625 [==============================] - 11s 17ms/step - loss: 0.0920 - acc: 0.9699 - val_loss: 0.5166 - val_acc: 0.8658\n"]}]},{"cell_type":"code","source":["#Evaluate\n","results = model.evaluate(test_data, test_labels)\n","print(results)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fp33OG0TTGCP","executionInfo":{"status":"ok","timestamp":1676485103996,"user_tz":180,"elapsed":5122,"user":{"displayName":"santiago lucero","userId":"03286679709682630982"}},"outputId":"617628d6-686f-466a-db43-8302246491cf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["782/782 [==============================] - 5s 6ms/step - loss: 0.6211 - acc: 0.8428\n","[0.6211398243904114, 0.8427600264549255]\n"]}]},{"cell_type":"code","source":["#Making predictions\n","#Since our reviews are encoded well need to convert any review that we write into that form so the network can understand it. To do that well load the encodings from the dataset and use them to encode our own data.\n","word_index = imdb.get_word_index()\n","#check if the words are in our vocabulary\n","\n","def encode_text(text): #convert the word in tokens\n","  tokens = keras.preprocessing.text.text_to_word_sequence(text)\n","  tokens = [word_index[word] if word in word_index else 0 for word in tokens]\n","  return pad_sequences([tokens], MAXLEN)[0]\n","\n","text = \"that movie was just amazing, so amazing\"\n","encoded = encode_text(text)\n","print(encoded)"],"metadata":{"id":"bsAscBIqTVpY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# while were at it lets make a decode function\n","\n","reverse_word_index = {value: key for (key, value) in word_index.items()}\n","\n","def decode_integers(integers):\n","    PAD = 0\n","    text = \"\"\n","    for num in integers:\n","      if num != PAD:\n","        text += reverse_word_index[num] + \" \"\n","\n","    return text[:-1]\n","  \n","print(decode_integers(encoded))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O5Ev51HiUpyg","executionInfo":{"status":"ok","timestamp":1676485565526,"user_tz":180,"elapsed":3,"user":{"displayName":"santiago lucero","userId":"03286679709682630982"}},"outputId":"ad82bf0d-fb2f-4b12-c76c-11dd4fe697a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["that movie was just amazing so amazing\n"]}]},{"cell_type":"code","source":["# now time to make a prediction\n","\n","def predict(text):\n","  encoded_text = encode_text(text)\n","  pred = np.zeros((1,250)) #blank\n","  pred[0] = encoded_text #insert our one entry into this\n","  result = model.predict(pred)  #model predict that array\n","  print(result[0]) #\n","\n","positive_review = \"That movie was! really loved it and would great watch it again because it was amazingly great\"\n","predict(positive_review)\n","\n","negative_review = \"that movie really sucked. I hated it and wouldn't watch it again. Was one of the worst things I've ever watched\"\n","predict(negative_review)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"36Y2HkMGUu0-","executionInfo":{"status":"ok","timestamp":1676485901265,"user_tz":180,"elapsed":430,"user":{"displayName":"santiago lucero","userId":"03286679709682630982"}},"outputId":"10980875-1cb0-44dc-bf74-a42977bcd34b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 352ms/step\n","[0.9298646]\n","1/1 [==============================] - 0s 21ms/step\n","[0.1962332]\n"]}]},{"cell_type":"code","source":["#RNN play generate\n","#will try to predict next character in the sentences\n","\n","%tensorflow_version 2.x  # this line is not required unless you are in a notebook\n","from keras.preprocessing import sequence\n","import keras\n","import tensorflow as tf\n","import os\n","import numpy as np"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vkKxa4z29uWj","executionInfo":{"status":"ok","timestamp":1676580320206,"user_tz":180,"elapsed":3692,"user":{"displayName":"santiago lucero","userId":"03286679709682630982"}},"outputId":"a86f0308-d47b-4e2e-fb36-9651940c1246"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"]}]},{"cell_type":"code","source":["path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KBvXNbXi-XIG","executionInfo":{"status":"ok","timestamp":1676580323107,"user_tz":180,"elapsed":770,"user":{"displayName":"santiago lucero","userId":"03286679709682630982"}},"outputId":"979aab7c-8f5c-46d5-c919-0a078011478f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n","1115394/1115394 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"code","source":["#import any local text\n","from google.colab import files\n","path_to_file = list(files.upload().keys())[0]"],"metadata":{"id":"ijHxBtoL-Y_a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Read, then decode for py2 compat.\n","text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n","# length of text is the number of characters in it\n","print ('Length of text: {} characters'.format(len(text)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xr8tu_KH-p8P","executionInfo":{"status":"ok","timestamp":1676580410232,"user_tz":180,"elapsed":3,"user":{"displayName":"santiago lucero","userId":"03286679709682630982"}},"outputId":"fbe684c2-599b-422d-ac93-f7d85bf10d20"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Length of text: 1115394 characters\n"]}]},{"cell_type":"code","source":["# Take a look at the first 250 characters in text\n","print(text[:250])"],"metadata":{"id":"K-nYsEjQ-xdX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Encoding, need to convert the text to numbers\n","vocab = sorted(set(text))\n","# Creating a mapping from unique characters to indices\n","char2idx = {u:i for i, u in enumerate(vocab)}\n","idx2char = np.array(vocab)\n","\n","def text_to_int(text):\n","  return np.array([char2idx[c] for c in text]) #every single character to int  and put it on the list\n","\n","text_as_int = text_to_int(text)"],"metadata":{"id":"k1tD191x-_EP","executionInfo":{"status":"ok","timestamp":1676580646479,"user_tz":180,"elapsed":291,"user":{"displayName":"santiago lucero","userId":"03286679709682630982"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# lets look at how part of our text is encoded\n","print(\"Text:\", text[:13])\n","print(\"Encoded:\", text_to_int(text[:13]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C3ZipI8S_wZA","executionInfo":{"status":"ok","timestamp":1676580687840,"user_tz":180,"elapsed":762,"user":{"displayName":"santiago lucero","userId":"03286679709682630982"}},"outputId":"ad5a8500-3f66-46cc-e639-5664f3a19e26"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Text: First Citizen\n","Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"]}]},{"cell_type":"code","source":["#function that can convert our numeric values to text\n","def int_to_text(ints):\n","  try:\n","    ints = ints.numpy()\n","  except:\n","    pass\n","  return ''.join(idx2char[ints])\n","\n","print(int_to_text(text_as_int[:13]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vin3cIph_3W1","executionInfo":{"status":"ok","timestamp":1676580718143,"user_tz":180,"elapsed":270,"user":{"displayName":"santiago lucero","userId":"03286679709682630982"}},"outputId":"d0dc11d5-39f8-4301-a4d4-8b5a8f75da49"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["First Citizen\n"]}]},{"cell_type":"code","source":["#Create training examples\n","# smth input: Hell | output: ello\n","\n","seq_length = 100  # length of sequence for a training example\n","examples_per_epoch = len(text)//(seq_length+1)\n","\n","# Create training examples / targets\n","char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int) #string data set to char"],"metadata":{"id":"e3wMeRTVAA1q","executionInfo":{"status":"ok","timestamp":1676580902161,"user_tz":180,"elapsed":277,"user":{"displayName":"santiago lucero","userId":"03286679709682630982"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["sequences = char_dataset.batch(seq_length+1, drop_remainder=True) #take the entire data and batch it into length of 101"],"metadata":{"id":"7QxPUD_NATNX","executionInfo":{"status":"ok","timestamp":1676580836652,"user_tz":180,"elapsed":287,"user":{"displayName":"santiago lucero","userId":"03286679709682630982"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["def split_input_target(chunk):  # for the example: hello\n","    input_text = chunk[:-1]  # hell\n","    target_text = chunk[1:]  # ello\n","    return input_text, target_text  # hell, ello\n","\n","dataset = sequences.map(split_input_target)  # we use map to apply the above function to every entry"],"metadata":{"id":"BiZIllzcA6Vk","executionInfo":{"status":"ok","timestamp":1676580992105,"user_tz":180,"elapsed":271,"user":{"displayName":"santiago lucero","userId":"03286679709682630982"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["\n","for x, y in dataset.take(2):\n","  print(\"\\n\\nEXAMPLE\\n\")\n","  print(\"INPUT\")\n","  print(int_to_text(x))\n","  print(\"\\nOUTPUT\")\n","  print(int_to_text(y))"],"metadata":{"id":"3g78CSOlBC4w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["BATCH_SIZE = 64\n","VOCAB_SIZE = len(vocab)  # vocab is number of unique characters\n","EMBEDDING_DIM = 256\n","RNN_UNITS = 1024\n","\n","# Buffer size to shuffle the dataset\n","# (TF data is designed to work with possibly infinite sequences,\n","# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n","# it maintains a buffer in which it shuffles elements).\n","BUFFER_SIZE = 10000\n","\n","data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"],"metadata":{"id":"kqLXtcQIBrcd","executionInfo":{"status":"ok","timestamp":1676581468212,"user_tz":180,"elapsed":261,"user":{"displayName":"santiago lucero","userId":"03286679709682630982"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["#Build the model\n","#pass batches of size 64,  but then this model we be saved and going to use witch batches of size 1 \n","def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n","  model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n","                              batch_input_shape=[batch_size, None]), #none, dont know how long the batch size is going to be\n","    tf.keras.layers.LSTM(rnn_units,\n","                        return_sequences=True,#need output from every single time\n","                        stateful=True,\n","                        recurrent_initializer='glorot_uniform'),\n","    tf.keras.layers.Dense(vocab_size) #amounf of nodes in it rqueal to the amoun of characters in the vocabulary, probability distribution of that character comes next\n","  ])\n","  return model\n","\n","model = build_model(VOCAB_SIZE,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n","model.summary()"],"metadata":{"id":"h3n6YvxUCH3Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#pass inputs of 64 entries with sequences of lenght 100 as a training data\n","#later on we are going to pass entris of 1"],"metadata":{"id":"mXTZnLnzEJUR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for input_example_batch, target_example_batch in data.take(1):\n","  example_batch_predictions = model(input_example_batch)  # ask our model for a prediction on our first batch of training data (64 entries)\n","  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")  # print out the output shape\n","  #every prediction its going to contain 65 numbers, and thats gooing to be the probability of every one of those caracters occurring\n","  "],"metadata":{"id":"wjEQEDQqJNds"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# we can see that the predicition is an array of 64 arrays, one for each entry in the batch\n","print(len(example_batch_predictions))\n","print(example_batch_predictions)"],"metadata":{"id":"PhpMvxE5Jmko"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# lets examine one prediction\n","pred = example_batch_predictions[0]\n","print(len(pred))\n","print(pred)\n","#every single training example has 100 putputs\n","# notice this is a 2d array of length 100, where each interior array is the prediction for the next character at each time step"],"metadata":{"id":"YddqLLvRJ7RC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# and finally well look at a prediction at the first timestep\n","time_pred = pred[0]\n","print(len(time_pred))\n","print(time_pred)\n","# and of course its 65 values representing the probabillity of each character occuring next"],"metadata":{"id":"QiEGsPXtKW0H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# If we want to determine the predicted character we need to sample the output distribution (pick a value based on probabillity)\n","sampled_indices = tf.random.categorical(pred, num_samples=1)\n","\n","# now we can reshape that array and convert all the integers to numbers to see the actual characters\n","sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n","predicted_chars = int_to_text(sampled_indices)\n","#sampling is pick a character based on a probability distribution, it doesn't guarantee that the caracter with the highest probability will be picked, it just uses those probabilities to pick it \n","predicted_chars  # and this is what the model predicted for training sequence 1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"V-N32LghKe2I","executionInfo":{"status":"ok","timestamp":1676583722229,"user_tz":180,"elapsed":276,"user":{"displayName":"santiago lucero","userId":"03286679709682630982"}},"outputId":"653a929f-5307-4017-886e-766db01e4660"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"YiPRqKGbQo!F&'JKd'!tEQnwD::mKyjPmbZGcDFgqLfKhW: s:VIUOpa P3RmNjzmJ'pl&P?WK,FvJZMWRWn-suc3xV:C:ReFIbx\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["#Loss function \n","#need to create a loss function that can compare that output to the expected output and give us some numeric value representing how close the two were.\n","def loss(labels, logits):\n","  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"],"metadata":{"id":"wCHqRnHGLaWz","executionInfo":{"status":"ok","timestamp":1676583762434,"user_tz":180,"elapsed":2,"user":{"displayName":"santiago lucero","userId":"03286679709682630982"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["#Compile\n","model.compile(optimizer='adam', loss=loss)"],"metadata":{"id":"tXNoUbAdLeQG","executionInfo":{"status":"ok","timestamp":1676583819929,"user_tz":180,"elapsed":2,"user":{"displayName":"santiago lucero","userId":"03286679709682630982"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["#Creatin checkpoints \n","#This will allow us to load our model from a checkpoint and continue training it.\n","# Directory where the checkpoints will be saved\n","checkpoint_dir = './training_checkpoints'\n","# Name of the checkpoint files\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n","\n","checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n","    filepath=checkpoint_prefix,\n","    save_weights_only=True)"],"metadata":{"id":"ZqzIk0DaLtUZ","executionInfo":{"status":"ok","timestamp":1676583854057,"user_tz":180,"elapsed":291,"user":{"displayName":"santiago lucero","userId":"03286679709682630982"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["history = model.fit(data, epochs=10, callbacks=[checkpoint_callback])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vmWN4eXDL8xV","executionInfo":{"status":"ok","timestamp":1676584121959,"user_tz":180,"elapsed":166634,"user":{"displayName":"santiago lucero","userId":"03286679709682630982"}},"outputId":"f5cf0a8f-f719-40a1-ece1-2367616d29e3"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","172/172 [==============================] - 14s 68ms/step - loss: 1.4068\n","Epoch 2/10\n","172/172 [==============================] - 14s 68ms/step - loss: 1.3524\n","Epoch 3/10\n","172/172 [==============================] - 14s 69ms/step - loss: 1.3084\n","Epoch 4/10\n","172/172 [==============================] - 14s 69ms/step - loss: 1.2691\n","Epoch 5/10\n","172/172 [==============================] - 14s 70ms/step - loss: 1.2321\n","Epoch 6/10\n","172/172 [==============================] - 15s 74ms/step - loss: 1.1963\n","Epoch 7/10\n","172/172 [==============================] - 15s 71ms/step - loss: 1.1594\n","Epoch 8/10\n","172/172 [==============================] - 14s 70ms/step - loss: 1.1236\n","Epoch 9/10\n","172/172 [==============================] - 14s 72ms/step - loss: 1.0830\n","Epoch 10/10\n","172/172 [==============================] - 16s 71ms/step - loss: 1.0440\n"]}]},{"cell_type":"code","source":["#Rebuild the model with batch size 1\n","model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"],"metadata":{"id":"45N0oF8GMTF9","executionInfo":{"status":"ok","timestamp":1676584186036,"user_tz":180,"elapsed":272,"user":{"displayName":"santiago lucero","userId":"03286679709682630982"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n","model.build(tf.TensorShape([1, None])) #Extpect the input 1 and then none dont know what the next input dimension will be"],"metadata":{"id":"LRcldN33MeS2","executionInfo":{"status":"ok","timestamp":1676584370030,"user_tz":180,"elapsed":297,"user":{"displayName":"santiago lucero","userId":"03286679709682630982"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["#Can load any checkpoint we want by specifying the exact file to load.\n","checkpoint_num = 10\n","model.load_weights(tf.train.load_checkpoint(\"./training_checkpoints/ckpt_\" + str(checkpoint_num)))\n","model.build(tf.TensorShape([1, None]))"],"metadata":{"id":"FvtKvTBiNbkm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate_text(model, start_string):\n","  # Evaluation step (generating text using the learned model)\n","\n","  # Number of characters to generate\n","  num_generate = 800\n","\n","  # Converting our start string to numbers (vectorizing)\n","  input_eval = [char2idx[s] for s in start_string]\n","  input_eval = tf.expand_dims(input_eval, 0)\n","\n","  # Empty string to store our results\n","  text_generated = []\n","\n","  # Low temperatures results in more predictable text.\n","  # Higher temperatures results in more surprising text.\n","  # Experiment to find the best setting.\n","  temperature = 1.0\n","\n","  # Here batch size == 1\n","  model.reset_states()\n","  for i in range(num_generate):\n","      predictions = model(input_eval)\n","      # remove the batch dimension\n","    \n","      predictions = tf.squeeze(predictions, 0)\n","\n","      # using a categorical distribution to predict the character returned by the model\n","      predictions = predictions / temperature\n","      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n","\n","      # We pass the predicted character as the next input to the model\n","      # along with the previous hidden state\n","      input_eval = tf.expand_dims([predicted_id], 0)\n","\n","      text_generated.append(idx2char[predicted_id])\n","\n","  return (start_string + ''.join(text_generated))"],"metadata":{"id":"7EhiPJ2tN2rz","executionInfo":{"status":"ok","timestamp":1676584386326,"user_tz":180,"elapsed":3,"user":{"displayName":"santiago lucero","userId":"03286679709682630982"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["inp = input(\"Type a starting string: \")\n","print(generate_text(model, inp))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i0bYNPfNN7D_","executionInfo":{"status":"ok","timestamp":1676584423585,"user_tz":180,"elapsed":19594,"user":{"displayName":"santiago lucero","userId":"03286679709682630982"}},"outputId":"0df0cde0-ff0c-444b-8847-7386f7bf33f5"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["Type a starting string: romeo\n","romeon,\n","But ne'er come to my assishance that he cap'tow,\n","All to thy favour and my mother doth,\n","With that now I must be condemn'd by consul.\n","\n","JULIET:\n","Come, come, tell thee, Signior Baptista:\n","Why, here's it speak.\n","\n","Clown:\n","His bloody days condemned customing modesty joy\n","To the ot for as this brand when her mad\n","When he did sind them bows, post he thence,\n","My friend, my lord, to wed with all my love;\n","Look to the roof drink butching. Pray you,\n","The late ye untimely but a little world,\n","In your commission: let's away my horse.\n","God save the boyou born to bear wearest: therefore,\n","' good sooner and to rave hrost\n","As that evilar earth?\n","Her sons and peace, it press'd to do set up\n","The least is merit; yet my father's bosom live we think; who, if you rave med!\n","\n","RICHARD:\n","The crding open truth of my pass, when of t\n"]}]}]}